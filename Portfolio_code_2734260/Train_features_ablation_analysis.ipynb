{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell imports all the modules we'll need. Make sure to run this once before running the other cells\n",
    "#sklearn is scikit-learn\n",
    "\n",
    "import sklearn\n",
    "import csv\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classifier\n",
    "\n",
    "We will first walk through the process of creating and evaluating a simple classifier that only uses the token itself as a feature. In the next step, we will run evaluations on this basic system.\n",
    "\n",
    "This is generally a good way to start experimenting: first walk through the entire experimental process with a very basic, easy to create system to see if everything works, there are no problems with the data etc. You can then build up from there towards a more sophististicated system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = 'data/conll2003.dev-preprocessed.conll'\n",
    "gold_train = 'data/conll2003.train-preprocessed.conll'\n",
    "    \n",
    "def create_vectorizer_and_classifier(features, labels, modelname ='logreg'):\n",
    "    '''\n",
    "    Function that takes feature-value pairs and gold labels as input and trains a logistic regression classifier\n",
    "    \n",
    "    :param features: feature-value pairs\n",
    "    :param labels: gold labels\n",
    "    :type features: a list of dictionaries\n",
    "    :type labels: a list of strings\n",
    "    \n",
    "    :return lr_classifier: a trained LogisticRegression classifier\n",
    "    :return vec: a DictVectorizer to which the feature values are fitted. \n",
    "    '''\n",
    "    \n",
    "    if modelname ==  'logreg':\n",
    "        vec = DictVectorizer()\n",
    "    #fit creates a mapping between observed feature values and dimensions in a one-hot vector, transform represents the current values as a vector \n",
    "        tokens_vectorized = vec.fit_transform(features)\n",
    "        classifier = LogisticRegression(solver='saga')\n",
    "        classifier.fit(tokens_vectorized, labels)\n",
    "        \n",
    "    elif modelname ==  'NB':\n",
    "        # TIP: you may need to solve this: https://stackoverflow.com/questions/61814494/what-is-this-warning-convergencewarning-lbfgs-failed-to-converge-status-1\n",
    "        classifier = MultinomialNB()\n",
    "        vec = DictVectorizer()\n",
    "        tokens_vectorized = vec.fit_transform(features)\n",
    "        classifier.fit(tokens_vectorized, labels)\n",
    "    \n",
    "    elif modelname ==  'SVM':\n",
    "        # TIP: you may need to solve this: https://stackoverflow.com/questions/61814494/what-is-this-warning-convergencewarning-lbfgs-failed-to-converge-status-1\n",
    "        classifier = svm.LinearSVC(max_iter=2000)\n",
    "        vec = DictVectorizer()\n",
    "        tokens_vectorized = vec.fit_transform(features)\n",
    "        classifier.fit(tokens_vectorized, labels)\n",
    "        \n",
    "\n",
    "    \n",
    "    return classifier, vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluation\n",
    "\n",
    "We will now run a basic evaluation of the system on a test file. \n",
    "Two important properties of the test file:\n",
    "\n",
    "1. the test file and training file are independent sets (if they contain identical examples, this is coincidental)\n",
    "2. the test file is preprocessed in the exact same way as the training file \n",
    "\n",
    "The first function runs our classifier on the test data.\n",
    "\n",
    "The second function prints out a confusion matrix (comparing predictions and gold labels per class). \n",
    "You can find more information on confusion matrices here: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "\n",
    "The third function prints out the macro precision, recall and f-score of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out a confusion matrix\n",
    "    \n",
    "    :param predictions: predicted labels\n",
    "    :param goldlabels: gold standard labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    #based on example from https://datatofish.com/confusion-matrix-python/ \n",
    "    data = {'Gold':    goldlabels, 'Predicted': predictions    }\n",
    "    df = pd.DataFrame(data, columns=['Gold','Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print (confusion_matrix)\n",
    "\n",
    "  \n",
    "\n",
    "def print_precision_recall_fscore(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out precision, recall and f-score\n",
    "    \n",
    "    :param predictions: predicted output by classifier\n",
    "    :param goldlabels: original gold labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    precision = metrics.precision_score(y_true=goldlabels,\n",
    "                        y_pred=predictions,\n",
    "                        average='macro')\n",
    "\n",
    "    recall = metrics.recall_score(y_true=goldlabels,\n",
    "                     y_pred=predictions,\n",
    "                     average='macro')\n",
    "\n",
    "\n",
    "    fscore = metrics.f1_score(y_true=goldlabels,\n",
    "                 y_pred=predictions,\n",
    "                 average='macro')\n",
    "\n",
    "    print('P:', precision, 'R:', recall, 'F1:', fscore)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_annotation(inputfile,annotationcolumn):\n",
    "    conll_input = pd.read_csv(inputfile, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "    annotations = conll_input[annotationcolumn].tolist()\n",
    "    return annotations\n",
    "\n",
    "inputfile1 = 'data/conll2003.dev.conll_revised'\n",
    "inputfile2 = 'data/conll2003.dev.crfconll_revised'\n",
    "goldlabels = crf_annotation(inputfile1, 'gold')\n",
    "predictions = crf_annotation(inputfile2, 'predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1545       4    104     24      2       0      5      1    152\n",
      "B-MISC         7     734     13     24      0       5      3      3    133\n",
      "B-ORG         44      15   1013     46      1       1     20      3    198\n",
      "B-PER         21       5     76   1469      1       5     13      8    244\n",
      "I-LOC          4       0      1      0    195       1     26     19     11\n",
      "I-MISC         1      18      0      5      4     224     11     26     57\n",
      "I-ORG         18       1     17      4     17       5    553     45     91\n",
      "I-PER          0       0      1      6      0       4     13   1241     42\n",
      "O              8      15     34     26     94      20     63     48  42451\n",
      "P: 0.8557823696236343 R: 0.8083171677366344 F1: 0.8282584862394202\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract extensive features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "# the functions with multiple features and analysis\n",
    "\n",
    "#defines the column in which each feature is located (note: you can also define headers and use csv.DictReader)\n",
    "feature_to_index = {'Token': 0, 'Pos': 1,'Chunklabel': 2,'Prevtoken': 4,'nexttoken': 5,'Prevpos': 6, 'nextpos': 7, 'capital': 8}\n",
    "\n",
    "\n",
    "def extract_features_and_gold_labels(conllfile, selected_features):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    count =0\n",
    "    for row in csvreader:\n",
    "        if count == 0:\n",
    "            pass\n",
    "            count +=1\n",
    "        else:    \n",
    "        #I preprocessed the file so that all rows with instances should contain 10 values, the others are empty lines indicating the beginning of a sentence\n",
    "            if len(row) == 10:\n",
    "                #structuring feature value pairs as key-value pairs in a dictionary\n",
    "                #the first column in the conll file represents tokens\n",
    "                feature_value = {}\n",
    "                for feature_name in selected_features:\n",
    "                    row_index = feature_to_index.get(feature_name)\n",
    "                    feature_value[feature_name] = row[row_index]\n",
    "                features.append(feature_value)\n",
    "                #The last column provides the gold label (= the correct answer). \n",
    "                labels.append(row[-1])\n",
    "                count +=1\n",
    "    return features, labels\n",
    "\n",
    "def get_predicted_and_gold_labels(testfile, vectorizer, classifier, selected_features):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    features, goldlabels = extract_features_and_gold_labels(testfile, selected_features)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(features)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "all_features = ['Token','Prevtoken','capital','Pos','Chunklabel', 'Prevpos','nexttoken','nextpos']\n",
    "\n",
    "sparse_feature_reps, labels = extract_features_and_gold_labels(gold_train, all_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(sparse_feature_reps, labels, modelname ='logreg')\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(gold, vectorizer, lr_classifier, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ablation Analysis\n",
    "\n",
    "We use the following codes to investigate the performance on different features and write the prediction result into csv file.\n",
    "You can uncomment the code to see the test result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(goldlabels, outputfile, predictions):\n",
    "    \n",
    "    \"\"\"\n",
    "    :param model: the model returned from three classifiers \n",
    "    :param vec: the feature representations transformed by gold features and annotations. \n",
    "    :param inputdata: a list of predicted features from system file \n",
    "    :param outputfile: the path to our outputfile\n",
    "    \n",
    "    The function utilized the pre-defined classifier to predict the annotations from the system file\n",
    "    (inputdata), and for each pair of testing and predicted features and annotation, the function writes\n",
    "    them into csv file.\n",
    "    \"\"\"\n",
    "    with open (outputfile, 'w', newline='', encoding = 'utf-8') as outfile:\n",
    "        count = 0\n",
    "        for label in goldlabels:\n",
    "            \n",
    "            if count == 0:\n",
    "                \n",
    "                outfile.write('gold'+'\\t'+'predict'+'\\n')\n",
    "                outfile.write(label.rstrip('\\n') + '\\t'  + predictions[count] + '\\n')    \n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                    \n",
    "                outfile.write(label.rstrip('\\n') + '\\t'  + predictions[count] + '\\n')  \n",
    "                    \n",
    "            count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "# example of system with just one additional feature\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "\n",
    "gold = 'data/conll2003.dev-preprocessed.conll'\n",
    "gold_train = 'data/conll2003.train-preprocessed.conll'\n",
    "\n",
    "selected_features = ['Token','Prevtoken','nexttoken','Pos', 'Prevpos','nextpos', 'Chunklabel', 'capital']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(gold_train, selected_features)\n",
    "\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels, modelname ='logreg')\n",
    "\n",
    "predictions_log, goldlabels_log = get_predicted_and_gold_labels(gold, vectorizer, lr_classifier, selected_features)\n",
    "# you can uncomment the lines below to test the result \n",
    "# print_confusion_matrix(predictions_log, goldlabels_log)\n",
    "# print_precision_recall_fscore(predictions_log, goldlabels_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels, modelname ='SVM')\n",
    "predictions_svm, goldlabels_svm = get_predicted_and_gold_labels(gold, vectorizer, svm_classifier, selected_features)\n",
    "# you can uncomment the lines below to test the result \n",
    "# print_confusion_matrix(predictions_svm, goldlabels_svm)\n",
    "# print_precision_recall_fscore(predictions_svm, goldlabels_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels, modelname ='NB')\n",
    "predictions_nb, goldlabels_nb = get_predicted_and_gold_labels(gold, vectorizer, NB_classifier, selected_features)\n",
    "# you can uncomment the lines below to test the result \n",
    "# print_confusion_matrix(predictions_nb, goldlabels_nb)\n",
    "# print_precision_recall_fscore(predictions_nb, goldlabels_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_data(goldlabels_log, 'data/conll2003.dev-preprocessed.predictlogistic', predictions_log)\n",
    "classify_data(goldlabels_svm, 'data/conll2003.dev-preprocessed.predictsvm', predictions_svm)\n",
    "classify_data(goldlabels_nb,'data/conll2003.dev-preprocessed.predictnb', predictions_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings as token representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step takes a while\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier....\n",
      "Running evaluation...\n"
     ]
    }
   ],
   "source": [
    "gold = 'data/conll2003.dev-preprocessed.conll'\n",
    "gold_train = 'data/conll2003.train-preprocessed.conll'\n",
    "\n",
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    count = 0\n",
    "    for row in csvreader:\n",
    "        if count ==0:\n",
    "            pass\n",
    "            count+=1\n",
    "        else:\n",
    "            if len(row) == 10:\n",
    "                if row[0] in word_embedding_model:\n",
    "                    vector = word_embedding_model[row[0]]\n",
    "                else:\n",
    "                    vector = [0]*300\n",
    "                features.append(vector)\n",
    "                labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def create_classifier(features, labels, model= 'logreg'):\n",
    "    '''\n",
    "    Function that creates classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if model == 'logreg':\n",
    "        classifier = LogisticRegression(solver='saga')\n",
    "        classifier.fit(features, labels)\n",
    "        \n",
    "    elif model == 'SVM':\n",
    "        classifier = svm.LinearSVC(max_iter=2000)\n",
    "        classifier.fit(features, labels)\n",
    "    \n",
    "    return classifier\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations, labels = extract_embeddings_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "# I printing announcements of where the code is at (since some of these steps take a while)\n",
    "\n",
    "print('Extracting dense features...')\n",
    "dense_feature_representations, labels = extract_embeddings_as_features_and_gold(gold_train,word_embedding_model)\n",
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, labels, model= 'SVM')\n",
    "print('Running evaluation...')\n",
    "predicted_token, gold_token = label_data_using_word_embeddings(gold, word_embedding_model, classifier)\n",
    "\n",
    "# you can uncomment the lines below to test the result \n",
    "# print_confusion_matrix(predicted_token, gold_token)\n",
    "# print_precision_recall_fscore(predicted_token, gold_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the preceding token\n",
    "\n",
    "We further include the preceding token as a feature in a similar way. We simply concatenate the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier...\n",
      "Running evaluation...\n"
     ]
    }
   ],
   "source": [
    "gold = 'data/conll2003.dev-preprocessed.conll'\n",
    "gold_train = 'data/conll2003.train-preprocessed.conll'\n",
    "\n",
    "def extract_embeddings_of_current_and_preceding_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    count = 0\n",
    "    for row in csvreader:\n",
    "        if count ==0:\n",
    "            pass\n",
    "            count+=1\n",
    "        else:    \n",
    "            if len(row) == 10:\n",
    "                if row[0] in word_embedding_model:\n",
    "                    vector1 = word_embedding_model[row[0]]\n",
    "                else:\n",
    "                    vector1 = [0]*300\n",
    "                if row[4] in word_embedding_model:\n",
    "                    vector2 = word_embedding_model[row[4]]\n",
    "                else:\n",
    "                    vector2 = [0]*300\n",
    "                features.append(np.concatenate((vector1,vector2)))\n",
    "                labels.append(row[-1])\n",
    "    return features, labels\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features (of current and preceding token) and gold labels from test data and runs a trained classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(features)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "print('Extracting dense features...')\n",
    "features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(gold_train,word_embedding_model)\n",
    "print('Training classifier...')\n",
    "\n",
    "classifier = create_classifier(features, labels, model='SVM')\n",
    "print('Running evaluation...')\n",
    "predicted_pretoken, gold_pretoken = label_data_using_word_embeddings_current_and_preceding(gold, word_embedding_model, classifier)\n",
    "\n",
    "# you can uncomment the lines below to test the result \n",
    "# print_confusion_matrix(predicted_pretoken, gold_pretoken)\n",
    "# print_precision_recall_fscore(predicted_pretoken, gold_pretoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A mixed system\n",
    "\n",
    "The code below combines traditional features with word embeddings adopting two different classifiers: logistic and SVM. Note that we only include features with a limited range of possible values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features...\n",
      "Training classifier....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the evaluation...\n"
     ]
    }
   ],
   "source": [
    "def extract_word_embedding(token, word_embedding_model):\n",
    "    '''\n",
    "    Function that returns the word embedding for a given token out of a distributional semantic model and a 300-dimension vector of 0s otherwise\n",
    "    \n",
    "    :param token: the token\n",
    "    :param word_embedding_model: the distributional semantic model\n",
    "    :type token: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :returns a vector representation of the token\n",
    "    '''\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0]*300\n",
    "    return vector\n",
    "\n",
    "\n",
    "def extract_feature_values(row, selected_features):\n",
    "    '''\n",
    "    Function that extracts feature value pairs from row\n",
    "    \n",
    "    :param row: row from conll file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_values = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values[feature_name] = row[r_index]\n",
    "        \n",
    "    return feature_values\n",
    "    \n",
    "    \n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Function that creates vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "        \n",
    "    \n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Function that takes sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    \n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        \n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))\n",
    "        combined_vectors.append(combined_vector)\n",
    "        \n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "def extract_traditional_features_and_embeddings_plus_gold_labels(conllfile, word_embedding_model, vectorizer=None):\n",
    "    '''\n",
    "    Function that extracts traditional features as well as embeddings and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    count = 0\n",
    "    for row in csvreader:\n",
    "        if count ==0:\n",
    "            pass\n",
    "            count +=1\n",
    "        else:\n",
    "            if len(row) == 10:\n",
    "                token_vector = extract_word_embedding(row[0], word_embedding_model)\n",
    "                pt_vector = extract_word_embedding(row[4], word_embedding_model)\n",
    "                dense_vectors.append(np.concatenate((token_vector,pt_vector)))\n",
    "                #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "                #we thus only use other features with limited values\n",
    "                other_features = extract_feature_values(row, ['capital','Pos','Chunklabel'])\n",
    "                traditional_features.append(other_features)\n",
    "                #adding gold label to labels\n",
    "                labels.append(row[-1])\n",
    "            \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer, labels\n",
    "\n",
    "def label_data_with_combined_features(testfile, classifier, vectorizer, word_embedding_model):\n",
    "    '''\n",
    "    Function that labels data with model using both sparse and dense features\n",
    "    '''\n",
    "    feature_vectors, vectorizer, goldlabels = extract_traditional_features_and_embeddings_plus_gold_labels(testfile, word_embedding_model, vectorizer)\n",
    "    predictions = classifier.predict(feature_vectors)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "\n",
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(gold_train, word_embedding_model)\n",
    "print('Training classifier....')\n",
    "lr_classifier = create_classifier(feature_vectors, gold_labels, model='logreg')\n",
    "print('Running the evaluation...')\n",
    "predictions_mix, goldlabels_mix = label_data_with_combined_features(gold, lr_classifier, vectorizer, word_embedding_model)\n",
    "# print_confusion_matrix(predictions_mix, goldlabels_mix)\n",
    "# print_precision_recall_fscore(predictions_mix, goldlabels_mix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features...\n",
      "Training classifier....\n",
      "Running the evaluation...\n"
     ]
    }
   ],
   "source": [
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(gold_train, word_embedding_model)\n",
    "print('Training classifier....')\n",
    "svm_classifier = create_classifier(feature_vectors, gold_labels, model='SVM')\n",
    "print('Running the evaluation...')\n",
    "predictions_mix_svm, goldlabels_mix_svm = label_data_with_combined_features(gold, svm_classifier, vectorizer, word_embedding_model)\n",
    "print_confusion_matrix(predictions_mix_svm, goldlabels_mix_svm)\n",
    "print_precision_recall_fscore(predictions_mix_svm, goldlabels_mix_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_data(gold_token, 'data/conll2003.dev-preprocessed.predict_w2vtoken', predicted_token)\n",
    "classify_data(gold_pretoken, 'data/conll2003.dev-preprocessed.predict_w2vpretoken', predicted_pretoken)\n",
    "classify_data(goldlabels_mix, 'data/conll2003.dev-preprocessed.predict_w2vmix', predictions_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
