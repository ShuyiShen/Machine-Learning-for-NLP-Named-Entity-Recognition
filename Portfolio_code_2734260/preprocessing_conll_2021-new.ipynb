{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List, Dict\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_feature(inputdata, outputfile):\n",
    "    \"\"\"\n",
    "    This function helps you extend the original features and \n",
    "    rmeove the prefix of NER annotations, converitng them into \n",
    "    the same format. The result returns a preprocessed csv file \n",
    "    that contains additional 5 features, including preceding, next\n",
    "    tokens and pos tags and capitals. \n",
    "    \n",
    "    :param inputdata: training file \n",
    "    :param outputfile: testing file \n",
    "    \n",
    "    \"\"\"\n",
    "    # we created lists to append the additional features\n",
    "    tokens = []\n",
    "    caps = [] # capitals \n",
    "    pos = [] # pos tags \n",
    "    simple_ner = [] # NER annotations without BIO format \n",
    "    \n",
    "    #  We load in training (conll2003 train) or testing file (conll2003 dev)\n",
    "    # and rotate the deque object to extract the preceding and next tokens and pos tags \n",
    "    # we removed the prefix of NER labels, and if the row in the training or testing file \n",
    "    # is empty, we append \"ENDSENTX\" to avoid the index error when writing in the file. \n",
    "    \n",
    "    with open(inputdata, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            \n",
    "            if len(components) > 0:\n",
    "                token = components[0]\n",
    "                pos_tag = components[1]\n",
    "                pos.append(pos_tag)\n",
    "                tokens.append(token)\n",
    "                upper = str(token[0].isupper())\n",
    "                caps.append(upper)\n",
    "                BIOner = components[3]\n",
    "                if BIOner[0] == 'B' or BIOner[0] == 'I':\n",
    "                    sn = BIOner[2:]\n",
    "                else:\n",
    "                    sn = BIOner\n",
    "                simple_ner.append(sn)\n",
    "                \n",
    "            else:\n",
    "                tokens.append(\"ENDSENTX\")\n",
    "                caps.append(\"ENDSENTX\")\n",
    "                pos.append(\"ENDSENTX\")\n",
    "                simple_ner.append(\"ENDSENTX\")\n",
    "                \n",
    "                \n",
    "    # we print out the lenth of additional features to double check if the overall number of our features is correct. \n",
    "    \n",
    "    print(len(tokens))\n",
    "\n",
    "    prev = deque(tokens)\n",
    "    prev.rotate(1)\n",
    "    prev_tokens = list(prev)\n",
    "\n",
    "    print(len(prev_tokens))\n",
    "\n",
    "    prev_ = deque(pos)\n",
    "    prev.rotate(1)\n",
    "    prev_pos = list(prev_)\n",
    "\n",
    "    print(len(prev_pos))\n",
    "\n",
    "    next_ = deque(tokens)\n",
    "    next_.rotate(-1)\n",
    "    next_tokens = list(next_)\n",
    "\n",
    "    print(len(next_tokens))\n",
    "\n",
    "    next_pos = deque(pos)\n",
    "    next_pos.rotate(-1)\n",
    "    next_pos = list(next_pos)\n",
    "\n",
    "    print(len(next_pos))\n",
    "    \n",
    "    \n",
    "    # we write in the additional features that were created above\n",
    "    # we also write in the header for the convenience to extract the column of a specific feature using pandas \n",
    "    \n",
    "    with open (outputfile, 'w', newline='', encoding = 'utf-8') as outfile:\n",
    "        count = 0\n",
    "        for line, prev_token, next_token, prevpos, nextpos, cap, NER in zip(open(inputdata, 'r'), prev_tokens, next_tokens, prev_pos, next_pos, caps, simple_ner):\n",
    "            \n",
    "            if count == 0:\n",
    "                \n",
    "                outfile.write('token' + '\\t' + 'pos' + '\\t' + 'chunk' + '\\t' + 'ner'+'\\t'+'prev_token'+'\\t'+'next_token'+'\\t'+'prevpos'+'\\t'+'nextpos'+'\\t'+'capital'+ '\\t'+'NER'+'\\n')\n",
    "                outfile.write(line.rstrip('\\n') + '\\t' + prev_token + '\\t' + next_token + '\\t' + prevpos+'\\t'+nextpos+'\\t'+cap + '\\t'+NER+'\\n')    \n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                if len(line.rstrip('\\n').split()) > 0:        \n",
    "                    outfile.write(line.rstrip('\\n') + '\\t' + prev_token + '\\t' + next_token + '\\t' + prevpos+'\\t'+nextpos+'\\t'+cap +'\\t' + NER + '\\n')\n",
    "                else:\n",
    "                    \n",
    "                    pass_ = (\"O\"+'\\t')*9 \n",
    "                    outfile.write(pass_+'O'+'\\n')\n",
    "                    \n",
    "            count +=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217662\n",
      "217662\n",
      "217662\n",
      "217662\n",
      "217662\n"
     ]
    }
   ],
   "source": [
    "inputdata = '/data/conll2003.train.conll'\n",
    "outputfile = 'data/conll2003.train-preprocessed.conll'\n",
    "\n",
    "write_feature(inputdata, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54612\n",
      "54612\n",
      "54612\n",
      "54612\n",
      "54612\n"
     ]
    }
   ],
   "source": [
    "inputdata = 'data/conll2003.dev.conll'\n",
    "outputfile = 'data/conll2003.dev-preprocessed.conll'\n",
    "\n",
    "write_feature(inputdata, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
